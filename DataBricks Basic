
To Read file :

    CSV read

          df1=spark.read.option('header',True).csv('dbfs:/mnt/outputsource/oudata/employees9956.csv')
          display(df1)
          
          df=spark.read.options(header='True',sep=',',inferSchema='True').csv('dbfs:/mnt/inputsource/indata/employees.csv')
          display(df)
          
          df3=spark.read.format('CSV').option('header',True).load('dbfs:/mnt/inputsource/indata/employees.csv')
          display(df3)
          
          df4=spark.read.format('CSV').option('header',True).option('inferSchema',True).option('sep',',').load('dbfs:/mnt/inputsource/indata/employees.csv')
          display(df4)
          
          df4=spark.read.format('CSV').options(header=True,inferSchema=True,sep=',').load('dbfs:/mnt/inputsource/indata/employees.csv')
          display(df4)
          
          df2=spark.read.csv('dbfs:/mnt/inputsource/indata/employees.csv',header=True,inferSchema=True)
          display(df2)


   Read Multiple CSV File :

          df=spark.read.format('csv').option('header',True).load(['dbfs:/mnt/loaddata/in/Temp1.csv','dbfs:/mnt/loaddata/in/Temp2.csv'])
          df=spark.read.format('csv').option('header',True).load('dbfs:/mnt/loaddata/in/')  // Only give upto folder name path  and change file format type according requirement.

    Parquet read

           df45=spark.read.option('header',True).parquet('dbfs:/mnt/outputsource/oudata/employees/')
           display(df45)

To write File :

         Write CSV :

             df.write.mode('overwrite').csv('dbfs:/mnt/outputsource/oudata/employees9956.csv')
    
             df6.write.mode('overwrite').csv('dbfs:/mnt/outputsource/oudata/employees.csv',header=True)

         Write Parquet :

             df12.write.option('header',True).parquet('dbfs:/mnt/outputsource/oudata/employees/')

=========================================================================================================================================


from pyspark.sql.types import StructType,StructField,StringType,IntegerType

schema=StructType([
StructField('id',IntegerType(),True),
StructField('firstname',StringType(),True),
StructField('lastname',StringType(),True)
])

data=[
(1,'Ramesh','Chavan'),
(2,'Kunal','Patil'),
(3,'Pranav','Patil')
]

df=spark.createDataFrame(data,schema)
df.coalesce(1).write.csv('dbfs:/mnt/outputsource/oudata/record/',header=True)

======================================================================================================================================================

from pyspark.sql.types import StructType,StructField,StringType,IntegerType

schema11=StructType([
StructField('id',IntegerType(),True),
StructField('firstname',StringType(),True),
StructField('lastname',StringType(),True)
])


df88=spark.read.schema(schema11).csv('dbfs:/mnt/outputsource/oudata/record/',header=True)
display(df88)

=========================================================================================================================================================

withColumn :

      ** withColumn is used to add new column and change existing column datatype

        from pyspark.sql.functions import lit,cast
        from pyspark.sql.types import IntegerType,StringType
        
        df=spark.read.csv('dbfs:/mnt/loaddata/in/Temp1.csv',header=True)
        df=df.withColumn('Dan',lit('111'))                                  // Add new column with name Dan
        df=df.withColumn('San',lit(df.ID*3))                                // Add new column with name San
        df=df.withColumn('Man',lit(df.ID*2).cast(StringType()))             // Add new column with name Man  and change datatype
        df=df.withColumn('Dan',df['Dan'].cast(IntegerType()))               // change datatype of existing column..
        display(df)   

withColumnRenamed : 

    ** withColumnRenamed is used to change existing column name


        df=df.withColumnRenamed('Man','Kan')                                // Existing column name renamed

==========================================================================================================================================================
filter :

    from pyspark.sql.types import IntegerType,StringType
    from pyspark.sql.functions import lit,cast,col
    
    df2=spark.read.csv('dbfs:/mnt/inputsource/indata/employees.csv',header=True)
    df3=df2.withColumn('SALARY',df2.SALARY.cast(IntegerType()))                   // Add New column with datatype
    display(df3)
    # df4=df3.filter((df3.JOB_ID=='ST_MAN') & (df3.SALARY >=8000))               // Use fileter
    df4=df3.filter((col('JOB_ID')=='ST_MAN') & (col('SALARY') >=8000))           // Use fileter with col
    display(df4)

=============================================================================================================================================================
startswith,endswith,contains :

    from pyspark.sql.functions import *
    
    
    df2=spark.read.csv('dbfs:/mnt/inputsource/indata/employees.csv',header=True)
    display(df2)
    df3=df2.filter(df2.FIRST_NAME.startswith('D'))
    df3=df2.filter(col('FIRST_NAME').startswith('D'))
    
    df3=df2.filter(df2.FIRST_NAME.endswith('d'))
    df3=df2.filter(col('FIRST_NAME').endswith('d'))
    
    df3=df2.filter((col('FIRST_NAME').contains('d')) & (col('PHONE_NUMBER').contains(423)))   // with col
    
    display(df3)
===============================================================================================================================================================
isin :

        from pyspark.sql.functions import *
        df3=spark.read.csv('dbfs:/mnt/inputsource/indata/employees.csv',header=True)
        # display(df3)
        df4=df3.filter(df3.JOB_ID.isin('SH_CLERK','IT_PROG'))
        df4=df3.filter(col('JOB_ID').isin('SH_CLERK','IT_PROG'))    // with col
        display(df4)

================================================================================================================================================================
select :

                from pyspark.sql.functions import *
                df3=spark.read.csv('dbfs:/mnt/inputsource/indata/employees.csv',header=True)
                #display(df3)
                df4=df3.select('FIRST_NAME','LAST_NAME')
                df4=df3.select('SALARY',(df3.SALARY*2).alias('Updated_SALARY'))
                df4=df3.select('SALARY',(col('SALARY')*2).alias('Updated_SALARY'))
                
                display(df4)


====================================================================================================================================================================
 
 when ...otherwise  :

            
            from pyspark.sql.functions import *
            df3=spark.read.csv('dbfs:/mnt/inputsource/indata/employees.csv',header=True)
            df4=df3.filter(df3.JOB_ID.isin('SH_CLERK','IT_PROG'))
            # display(df4)
            df5=df4.select('JOB_ID','SALARY',when(df4.JOB_ID=='SH_CLERK',(df4.SALARY+100))
                       .when(df4.JOB_ID=='IT_PROG',(df4.SALARY+200))
                       .otherwise(50).alias('UpdatedSalary')
                       )
            
            df5=df4.withColumn('UpdatedSalary',when(df4.JOB_ID=='SH_CLERK',(df4.SALARY+100))
                       .when(df4.JOB_ID=='IT_PROG',(df4.SALARY+200))
                       .otherwise(50)
                       )
            display(df5.select('JOB_ID','SALARY','UpdatedSalary'))

======================================================================================================================================================================
